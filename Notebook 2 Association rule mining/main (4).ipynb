{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association rule mining\n",
    "\n",
    "In this notebook, you'll implement the basic pairwise association rule mining algorithm.\n",
    "\n",
    "To keep the implementation simple, you will apply your implementation to a simplified dataset, namely, letters (\"items\") in words (\"receipts\" or \"baskets\"). Having finished that code, you will then apply that code to some grocery store market basket data. If you write the code well, it will not be difficult to reuse building blocks from the letter case in the basket data case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "main.global_imports"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cse6040_devkit.plugins\n",
      "cse6040_devkit.utils\n",
      "cse6040_devkit.utils\n"
     ]
    }
   ],
   "source": [
    "### Global imports\n",
    "import dill\n",
    "from cse6040_devkit import plugins, utils\n",
    "from cse6040_devkit.training_wheels import run_with_timeout, suppress_stdout\n",
    "import tracemalloc\n",
    "from time import time\n",
    "import re \n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from textwrap import dedent\n",
    "from operator import itemgetter\n",
    "\n",
    "utils.add_from_file('update_dd_checker', plugins)\n",
    "utils.add_from_file('gen_rule_str', utils)\n",
    "utils.add_from_file('print_rules', utils)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition\n",
    "\n",
    "Let's say you have a fragment of text in some language. You wish to know whether there are association rules among the letters that appear in a word. In this problem:\n",
    "\n",
    "- Words are \"receipts\"\n",
    "- Letters within a word are \"items\"\n",
    "\n",
    "You want to know whether there are _association rules_ of the form, $a \\implies b$, where $a$ and $b$ are letters. You will write code to do that by calculating for each rule its _confidence_, $\\mathrm{conf}(a \\implies b)$. \"Confidence\" will be another name for an estimate of the conditional probability of $b$ given $a$, or $\\mathrm{Pr}[b \\,|\\, a]$.\n",
    "\n",
    "## Sample text input\n",
    "\n",
    "Let's carry out this analysis on a \"dummy\" text fragment, which graphic designers refer to as the [_lorem ipsum_](https://en.wikipedia.org/wiki/Lorem_ipsum):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "tags": [
     "investigate_latin_text.preload_objects"
    ]
   },
   "outputs": [],
   "source": [
    "### Run Me!!!\n",
    "latin_text = utils.load_object_from_publicdata('latin_text')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "investigate_latin_text.prompt"
    ]
   },
   "source": [
    "### Exercise 0: (0 points)\n",
    "**investigate_latin_text**  \n",
    "\n",
    "**Example:** we have defined `investigate_latin_text` as follows:\n",
    "\n",
    "\n",
    "This is an ungraded exercise. There is no code to write.\n",
    "Instead, read the provided Latin text (Lorem Ipsum) and research its meaning.\n",
    "\n",
    "This function does not return any value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "tags": [
     "investigate_latin_text.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(latin_text)=<class 'str'>\n",
      "Length of latin_text: 1744 characters\n",
      "\n",
      "First 100 characters of latin_text:\n",
      "\n",
      "Sed ut perspiciatis, unde omnis iste natus error sit\n",
      "voluptatem accusantium doloremque laudantium, \n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 0  \n",
    "def investigate_latin_text():\n",
    "    print(f'{type(latin_text)=}')\n",
    "    print(f'Length of latin_text: {len(latin_text)} characters')\n",
    "    print()\n",
    "    print(f'First 100 characters of latin_text:\\n{latin_text[:100]}')\n",
    "\n",
    "### Demo function call\n",
    "investigate_latin_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "investigate_latin_text.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    " The test cell below will always pass. Please submit to collect your free points for investigate_latin_text (exercise 0).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_0",
     "locked": true,
     "points": 0,
     "solution": false
    },
    "tags": [
     "investigate_latin_text.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 0  \n",
    "\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Like most data in the real world, this dataset is noisy. It has both uppercase and lowercase letters, words have repeated letters, and there are all sorts of non-alphabetic characters. For our analysis, we should keep all the letters and spaces (so we can identify distinct words), but we should ignore case and ignore repetition within a word.\n",
    "\n",
    "For example, the eighth word of this text is \"error.\" As an _itemset_, it consists of the three unique letters, $\\{e, o, r\\}$. That is, treat the word as a set, meaning you only keep the unique letters.\n",
    "\n",
    "This itemset has three possible _itempairs_: $\\{e, o\\}$, $\\{e, r\\}$, and $\\{o, r\\}$.\n",
    "\n",
    "> Since sets are unordered, note that we would regard $\\{e, o\\} = \\{o, e\\}$, which is why we say there are only three itempairs, rather than six.\n",
    "\n",
    "Start by writing some code to help \"clean up\" the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "normalize_string.prompt"
    ]
   },
   "source": [
    "### Exercise 1: (2 points)\n",
    "**normalize_string**  \n",
    "\n",
    "**Your task:** define `normalize_string` as follows:\n",
    "\n",
    "\n",
    "Normalize the input string by lowercasing and removing non-alphabetic, non-whitespace characters.\n",
    "\n",
    "Args:\n",
    "\n",
    "- s (str): The input string to normalize.\n",
    "\n",
    "Returns:\n",
    "\n",
    "- str: A new string containing only lowercase alphabetic characters and whitespace characters, preserving the original order and spacing of retained characters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Clarification_. Scanning the sample text, `latin_text`, you may see things that look like special cases. For instance, `inci[di]dunt` and `[do]`. For these, simply remove the non-alphabetic characters and only separate the words if there is explicit whitespace.\n",
    ">\n",
    "> For instance, `inci[di]dunt` would become `incididunt` (as a single word) and `[do]` would become `do` as a standalone word because the original string has whitespace on either side. A period or comma without whitespace would, similarly, just be treated as a non-alphabetic character inside a word _unless_ there is explicit whitespace. So `e pluribus.unum basium` would become `e pluribusunum basium` even though your common-sense understanding might separate `pluribus` and `unum`.\n",
    ">\n",
    "> _Hint_. Regard as a whitespace character anything \"whitespace-like.\" That is, consider not just regular spaces, but also tabs, newlines, and perhaps others. To detect whitespaces easily, look for a \"high-level\" function that can help you do so rather than checking for literal space characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "tags": [
     "normalize_string.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_s_ex1: \n",
      "Sed ut perspiciatis, unde omnis iste natus error sit\n",
      "voluptatem accusantium doloremque laudantium, \n",
      "\n",
      "normalize_string(demo_s_ex1): \n",
      "sed ut perspiciatis unde omnis iste natus error sit\n",
      "voluptatem accusantium doloremque laudantium \n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 1  \n",
    "def normalize_string(s):\n",
    "        \n",
    "    assert type (s) is str\n",
    "    s = s.lower()\n",
    "\n",
    "    return re.sub(r'[^a-z\\s]','', s)\n",
    "        \n",
    "\n",
    "### Demo function call\n",
    "demo_s_ex1 = latin_text[:100]\n",
    "print(f'demo_s_ex1: {demo_s_ex1}\\n')\n",
    "print(f'normalize_string(demo_s_ex1): {normalize_string(demo_s_ex1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "normalize_string.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "demo_s_ex1: \n",
    "Sed ut perspiciatis, unde omnis iste natus error sit\n",
    "voluptatem accusantium doloremque laudantium, \n",
    "\n",
    "normalize_string(demo_s_ex1): \n",
    "sed ut perspiciatis unde omnis iste natus error sit\n",
    "voluptatem accusantium doloremque laudantium\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for normalize_string (exercise 1). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_1",
     "locked": true,
     "points": 2,
     "solution": false
    },
    "tags": [
     "normalize_string.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.14 seconds\n",
      "memory after test: 3.07 MB\n",
      "memory peak during test: 4.21 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 1  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=normalize_string,\n",
    "              ex_name='normalize_string',\n",
    "              key=b'uB5AD-6LZ4KH6PExkCGTzp065lKUINubYeq5q9rcV00=', \n",
    "              n_iter=30)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to normalize_string did not pass the test.'\n",
    "\n",
    "\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "get_normalized_words.prompt"
    ]
   },
   "source": [
    "### Exercise 2: (1 points)\n",
    "**get_normalized_words**  \n",
    "\n",
    "**Your task:** define `get_normalized_words` as follows:\n",
    "\n",
    "\n",
    "Normalize the input string and return a list of its words.\n",
    "\n",
    "Args:\n",
    "\n",
    "- s (str): The input string to process.\n",
    "\n",
    "Returns:\n",
    "- list of str: A list of words obtained from the normalized string.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- The \"normalization\" process involves lowercasing the string and removing non-alphabetic, non-whitespace characters. (You may find it helpful to call the `normalize_string` function defined earlier.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "tags": [
     "get_normalized_words.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_s_ex2: \n",
      "Sed ut perspiciatis, unde omnis \n",
      "\n",
      "get_normalized_words(demo_s_ex2)=['sed', 'ut', 'perspiciatis', 'unde', 'omnis']\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 2  \n",
    "def get_normalized_words(s):\n",
    "\n",
    "    assert type(s) is str\n",
    "    s = s.lower()\n",
    "    \n",
    "    return re.sub(r'[^a-z\\s]', '', s).split()\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "### Demo function call\n",
    "demo_s_ex2 = latin_text[:33]\n",
    "print(f'demo_s_ex2: {demo_s_ex2}\\n')\n",
    "print(f'{get_normalized_words(demo_s_ex2)=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "get_normalized_words.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "demo_s_ex2: \n",
    "Sed ut perspiciatis, unde omnis \n",
    "\n",
    "get_normalized_words(demo_s_ex2)=['sed', 'ut', 'perspiciatis', 'unde', 'omnis']\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for get_normalized_words (exercise 2). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_2",
     "locked": true,
     "points": 1,
     "solution": false
    },
    "tags": [
     "get_normalized_words.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.08 seconds\n",
      "memory after test: 0.05 MB\n",
      "memory peak during test: 1.41 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 2  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=get_normalized_words,\n",
    "              ex_name='get_normalized_words',\n",
    "              key=b'uB5AD-6LZ4KH6PExkCGTzp065lKUINubYeq5q9rcV00=', \n",
    "              n_iter=30)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to get_normalized_words did not pass the test.'\n",
    "\n",
    "\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "make_itemsets_unstructured_text.prompt"
    ]
   },
   "source": [
    "### Exercise 3: (2 points)\n",
    "**make_itemsets_unstructured_text**  \n",
    "\n",
    "**Your task:** define `make_itemsets_unstructured_text` as follows:\n",
    "\n",
    "\n",
    "Given unstructured text, return a list of sets corresponding to distinct letters in the normalized words in the text.\n",
    "\n",
    "Args:\n",
    "\n",
    "- text (str): The input unstructured text.\n",
    "\n",
    "Returns:\n",
    "\n",
    "- list of set: A list of sets, each containing the distinct letters of a normalized word.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- The output itemsets should appear in the same order as their corresponding words in the input text.\n",
    "- The normalization process involves lowercasing the text and removing non-alphabetic, non-whitespace characters. (You may find it helpful to call the `get_normalized_words` function defined earlier.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "tags": [
     "make_itemsets_unstructured_text.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_text_ex3: sed \tut, perspiciatis\n",
      " und.e omnis\n",
      "\n",
      "make_itemsets_unstructured_text(demo_text_ex3)=[{'s', 'e', 'd'}, {'u', 't'}, {'p', 'r', 'c', 'a', 's', 'e', 'i', 't'}, {'u', 'n', 'e', 'd'}, {'n', 'o', 's', 'i', 'm'}]\n"
     ]
    }
   ],
   "source": [
    "def make_itemsets_unstructured_text(text):\n",
    "    \n",
    "    #text = text.lower()\n",
    "    #text_modified = re.sub(r'[^a-z ]', '', text).split()\n",
    "    ##return text_modified\n",
    "    words = get_normalized_words(text)\n",
    "    list_set = [set(word) for word in words]\n",
    "    return list_set\n",
    "\n",
    "   \n",
    "    \n",
    "### Demo function call\n",
    "demo_text_ex3 = 'sed \\tut, perspiciatis\\n und.e omnis'\n",
    "print(f'demo_text_ex3: {demo_text_ex3}\\n')\n",
    "print(f'{make_itemsets_unstructured_text(demo_text_ex3)=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "make_itemsets_unstructured_text.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "demo_text_ex3: sed \tut, perspiciatis\n",
    " und.e omnis\n",
    "\n",
    "make_itemsets_unstructured_text(demo_text_ex3)=[{'d', 's', 'e'}, {'t', 'u'}, {'r', 's', 't', 'a', 'c', 'p', 'i', 'e'}, {'d', 'n', 'e', 'u'}, {'s', 'o', 'n', 'i', 'm'}]\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for make_itemsets_unstructured_text (exercise 3). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_3",
     "locked": true,
     "points": 2,
     "solution": false
    },
    "tags": [
     "make_itemsets_unstructured_text.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.07 seconds\n",
      "memory after test: 0.07 MB\n",
      "memory peak during test: 1.97 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 3  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=make_itemsets_unstructured_text,\n",
    "              ex_name='make_itemsets_unstructured_text',\n",
    "              key=b'uB5AD-6LZ4KH6PExkCGTzp065lKUINubYeq5q9rcV00=', \n",
    "              n_iter=30)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to make_itemsets_unstructured_text did not pass the test.'\n",
    "\n",
    "\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the basic algorithm\n",
    "\n",
    "Recall the pseudocode for the algorithm that Rachel and Rich derived together:\n",
    "\n",
    "![FindAssocRules (pseudocode)](https://ndownloader.figshare.com/files/7635700?private_link=3c473609741895a5cc2c)\n",
    "\n",
    "In the following series of exercises, let's implement this method. We'll build it \"bottom-up,\" first defining small pieces and working our way toward the complete algorithm. This method allows us to test each piece before combining them.\n",
    "\n",
    "Observe that the bulk of the work in this procedure is just updating these tables, $T$ and $C$. So your biggest implementation decision is how to store those. A good choice is to use a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: Default dictionaries\n",
    "\n",
    "Recall that the overall algorithm requires maintaining a table of item-pair (tuples) counts. It would be convenient to use a dictionary to store this table, where keys refer to item-pairs and the values are the counts.\n",
    "\n",
    "However, with Python's built-in dictionaries, you always to have to check whether a key exists before updating it. For example, consider this code fragment:\n",
    "\n",
    "```python\n",
    "D = {'existing-key': 5} # Dictionary with one key-value pair\n",
    "\n",
    "D['existing-key'] += 1 # == 6\n",
    "D['new-key'] += 1  # Error: 'new-key' does not exist!\n",
    "```\n",
    "\n",
    "The second attempt causes an error because `'new-key'` is not yet a member of the dictionary. So, a more correct approach would be to do the following:\n",
    "\n",
    "```python\n",
    "D = {'existing-key': 5} # Dictionary with one key-value pair\n",
    "\n",
    "if 'existing-key' not in D:\n",
    "    D['existing-key'] = 0\n",
    "D['existing-key'] += 1\n",
    "   \n",
    "if 'new-key' not in D:\n",
    "    D['new-key'] = 0\n",
    "D['new-key'] += 1\n",
    "```\n",
    "\n",
    "This pattern is so common that there is a special form of dictionary, called a _default dictionary_, which is available from the `collections` module: [`collections.defaultdict`](https://docs.python.org/3/library/collections.html?highlight=defaultdict#collections.defaultdict).\n",
    "\n",
    "When you create a default dictionary, you need to provide a \"factory\" function that the dictionary can use to create an initial value when the key does *not* exist. For instance, in the preceding example, when the key was not present the code creates a new key with the initial value of an integer zero (0). Indeed, this default value is the one you get when you call `int()` with no arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'existing-key': 6, 'new-key': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "D2 = defaultdict(int) # Empty dictionary\n",
    "\n",
    "D2['existing-key'] = 5 # Create one key-value pair\n",
    "\n",
    "D2['existing-key'] += 1 # Update\n",
    "D2['new-key'] += 1\n",
    "\n",
    "print(D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application in our algorithm: pair_counts\n",
    "\n",
    "We can use a default dictionary to represent the tables $T$ and $C$ from our algorithm. Let's start with $T$...\n",
    "\n",
    "_Let's say we have two itemsets:_\n",
    "\n",
    "```python\n",
    "itemsets = [set('go'), set('dog')]\n",
    "```\n",
    "\n",
    "_Let's set up the default dictionary_\n",
    "\n",
    "```python\n",
    "pair_counts = defaultdict(int)\n",
    "# pair_counts[(a, b)] = count of itemsets which contain both a and b.\n",
    "```\n",
    "\n",
    "_Now let's iterate through itemsets and update our table:_\n",
    "\n",
    "```python\n",
    "for itemset in itemsets:\n",
    "    update_pair_counts(pair_counts, itemset)\n",
    "    print(pair_counts)\n",
    "```\n",
    "\n",
    "_We would expect the first print to show a defaultdict containing:_\n",
    "\n",
    "```python\n",
    "{('g', 'o'): 1, ('o', 'g'): 1}\n",
    "```\n",
    "\n",
    "_We started with an empty `pair_counts`, and updated it with the distinct pairs of letters in the word \"go\"._\n",
    "\n",
    "_The second print should show the defaultdict now contains:_\n",
    "\n",
    "```python\n",
    "{('g', 'o'): 2, ('o', 'g'): 2, ('d', 'o'): 1, ('o', 'd'): 1, ('d', 'g'): 1, ('g', 'd'): 1}\n",
    "```\n",
    "\n",
    "_The pairs ('g', 'o') and ('o', 'g') are in 'dog', so those counts are incremented. The other pairs containing 'd' now have counts of 1._\n",
    "\n",
    "**_Now we need to implement `update_pair_counts`!_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "update_pair_counts.prompt"
    ]
   },
   "source": [
    "### Exercise 4: (2 points)\n",
    "**update_pair_counts**  \n",
    "\n",
    "**Your task:** define `update_pair_counts` as follows:\n",
    "\n",
    "\n",
    "Update the pair_counts default dictionary for all item pairs in the given itemset.\n",
    "\n",
    "Args:\n",
    "\n",
    "- pair_counts (defaultdict): The dictionary to update.\n",
    "- itemset (set): The set of items to consider.\n",
    "\n",
    "Returns:\n",
    "\n",
    "- None: The function updates the pair_counts dictionary in place.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- You may assume all items in the given itemset are distinct.\n",
    "- You may also assume the pair_counts dictionary is a default dictionary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**\n",
    "\n",
    "We have imported `combinations` from `itertools` (at the top of the notebook), which is quite useful for iterating over the possible pairs of items within an itemset. This is not the only way to solve the exercise, but we think it's the most straight-forward. \n",
    "\n",
    "Feel free to explore the `itertools` module or experiment on your own if you're interested in alternative ways of solving this part of the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "tags": [
     "update_pair_counts.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first_itemset: \n",
      "{('o', 'g'): 1, ('g', 'o'): 1}\n",
      "After second_itemset: \n",
      "{('o', 'g'): 2, ('g', 'o'): 2, ('o', 'd'): 1, ('d', 'o'): 1, ('g', 'd'): 1, ('d', 'g'): 1}\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 4  \n",
    "def update_pair_counts (pair_counts, itemset):\n",
    "    assert type (pair_counts) is defaultdict\n",
    "    for (a,b) in combinations(itemset, 2):\n",
    "            pair_counts[a,b] += 1\n",
    "            pair_counts[b,a] += 1\n",
    "    return pair_counts\n",
    "       \n",
    "        \n",
    "   \n",
    "        \n",
    "           \n",
    "   \n",
    "### Demo function call\n",
    "demo_pair_counts_ex4 = defaultdict(int)\n",
    "update_pair_counts(demo_pair_counts_ex4, set('go'))\n",
    "print(f'After first_itemset: \\n{dict(demo_pair_counts_ex4)}')\n",
    "update_pair_counts(demo_pair_counts_ex4, set('dog'))\n",
    "print(f'After second_itemset: \\n{dict(demo_pair_counts_ex4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "update_pair_counts.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "After first_itemset: \n",
    "{('g', 'o'): 1, ('o', 'g'): 1}\n",
    "After second_itemset: \n",
    "{('g', 'o'): 2, ('o', 'g'): 2, ('d', 'g'): 1, ('g', 'd'): 1, ('d', 'o'): 1, ('o', 'd'): 1}\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for update_pair_counts (exercise 4). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "The test uses items which are multi-character strings rather than individual letters. If your solution works for the longer strings, it will also work for the letters. However, the reverse may not be true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_4",
     "locked": true,
     "points": 2,
     "solution": false
    },
    "tags": [
     "update_pair_counts.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.74 seconds\n",
      "memory after test: 0.19 MB\n",
      "memory peak during test: 2.34 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 4  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=plugins.update_dd_checker(update_pair_counts),\n",
    "              ex_name='update_pair_counts',\n",
    "              key=b'uB5AD-6LZ4KH6PExkCGTzp065lKUINubYeq5q9rcV00=', \n",
    "              n_iter=30)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to update_pair_counts did not pass the test.'\n",
    "\n",
    "\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application in our algorithm: item_counts\n",
    "\n",
    "Same idea as above. The plan is to use a default dictionary to model the $C$ table.\n",
    "\n",
    "```python\n",
    "# initialize itemsets\n",
    "itemsets = [set('go'), set('dog')]\n",
    "\n",
    "# Create item_counts\n",
    "# item_counts[some_item] = number of itemsets containing the item `some_item` (whatever that may be)\n",
    "item_counts = defaultdict(int)\n",
    "\n",
    "# iterate and update\n",
    "for itemset in itemsets:\n",
    "    update_item_counts(item_counts, itemset)\n",
    "    print(item_counts)\n",
    "```\n",
    "\n",
    "We expect the prints to be:\n",
    "\n",
    "```\n",
    "{'g': 1, 'o': 1}\n",
    "{'g': 2, 'o': 2, 'd': 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "update_item_counts.prompt"
    ]
   },
   "source": [
    "### Exercise 5: (2 points)\n",
    "**update_item_counts**  \n",
    "\n",
    "**Your task:** define `update_item_counts` as follows:\n",
    "\n",
    "\n",
    "Update the item_counts default dictionary for all items in the given itemset.\n",
    "\n",
    "Args:\n",
    "\n",
    "- item_counts (defaultdict): The dictionary to update.\n",
    "- itemset (set): The set of items to consider.\n",
    "\n",
    "Returns:\n",
    "\n",
    "- None: The function updates the item_counts dictionary in place.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- You may assume all items in the given itemset are distinct.\n",
    "- You may also assume the item_counts dictionary is a default dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "tags": [
     "update_item_counts.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first_itemset: \n",
      "{'o': 1, 'g': 1}\n",
      "After second_itemset: \n",
      "{'o': 2, 'g': 2, 'd': 1}\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 5  \n",
    "def update_item_counts(item_counts, itemset):\n",
    "    assert type(item_counts) is defaultdict\n",
    "    \n",
    "    for i in itemset:\n",
    "        item_counts[i] +=1\n",
    "    \n",
    "    return item_counts\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "### Demo function call\n",
    "demo_item_counts_ex5 = defaultdict(int)\n",
    "update_item_counts(demo_item_counts_ex5, set('go'))\n",
    "print(f'After first_itemset: \\n{dict(demo_item_counts_ex5)}')\n",
    "update_item_counts(demo_item_counts_ex5, set('dog'))\n",
    "print(f'After second_itemset: \\n{dict(demo_item_counts_ex5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "update_item_counts.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "After first_itemset: \n",
    "{'g': 1, 'o': 1}\n",
    "After second_itemset: \n",
    "{'g': 2, 'o': 2, 'd': 1}\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for update_item_counts (exercise 5). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_5",
     "locked": true,
     "points": 2,
     "solution": false
    },
    "tags": [
     "update_item_counts.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.09 seconds\n",
      "memory after test: 0.03 MB\n",
      "memory peak during test: 1.39 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 5  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=plugins.update_dd_checker(update_item_counts),\n",
    "              ex_name='update_item_counts',\n",
    "              key=b'uB5AD-6LZ4KH6PExkCGTzp065lKUINubYeq5q9rcV00=', \n",
    "              n_iter=30)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to update_item_counts did not pass the test.'\n",
    "\n",
    "\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application in our algorithm: Confidence calculation\n",
    "\n",
    "After populating `pair_counts` and `item_counts` from a collection of \"receipts\", we have the data in a form where it can be analyzed. It's time to implement the pairwise association rules by calculating the confidence!\n",
    "\n",
    "Remember, $\\mathrm{conf}(a \\Rightarrow b)$ is $\\text{Pr}(b|a)$ - the probability that item $b$ is in a basket given that item $a$ is in the basket.\n",
    "\n",
    "We can estimate this with our \"counts\" dictionaries.\n",
    "\n",
    "`conf[(a, b)] = pair_counts[(a, b)] / item_counts[a]`\n",
    "\n",
    "If we choose `a` and `b` by iterating over the keys of `pair_counts`, we will have a confidence calculation for each pair of items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside\n",
    "\n",
    "Printing the confidence rules using the built-in `print` leads to some messy output. We have provided `utils.print_rules` to print them nicely in the demos for the exercises below. Remember that the rules being printed are still dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "create_rules_from_counts.prompt"
    ]
   },
   "source": [
    "### Exercise 6: (1 points)\n",
    "**create_rules_from_counts**  \n",
    "\n",
    "**Your task:** define `create_rules_from_counts` as follows:\n",
    "\n",
    "\n",
    "Create association rules from pair counts and item counts.\n",
    "\n",
    "Args:\n",
    "\n",
    "- pair_counts (dict): A dictionary mapping pairs (a, b) to their co-occurrence counts.\n",
    "- item_counts (dict): A dictionary mapping items to their individual occurrence counts.\n",
    "\n",
    "Returns:\n",
    "- dict: A dictionary mapping pairs (a, b) to their confidence values. In other words the pair (a,b) maps to $\\mathrm{conf}(a \\Rightarrow b)$.\n",
    "\n",
    "Note:\n",
    "- You may assume that for every (a, b) in pair_counts, a is present in item_counts with a non-zero count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "tags": [
     "create_rules_from_counts.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf(blue fish => red fish) = 0.818\n",
      "conf(one fish => red fish) = 0.809\n",
      "conf(two fish => red fish) = 0.797\n",
      "conf(blue fish => two fish) = 0.614\n",
      "conf(one fish => two fish) = 0.596\n",
      "conf(red fish => two fish) = 0.590\n",
      "conf(red fish => one fish) = 0.380\n",
      "conf(two fish => one fish) = 0.378\n",
      "conf(two fish => blue fish) = 0.365\n",
      "conf(blue fish => one fish) = 0.364\n",
      "conf(red fish => blue fish) = 0.360\n",
      "conf(one fish => blue fish) = 0.340\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 6  \n",
    "def create_rules_from_counts(pair_counts, item_counts):\n",
    "    rules = {} # (item_a, item_b) -> conf (item_a => item_b)\n",
    "    \n",
    "    for (a, b) in pair_counts:\n",
    "        \n",
    "        conf = pair_counts[(a, b)] / item_counts[a]\n",
    "        rules[(a,b)] = conf\n",
    "    return rules\n",
    "\n",
    "### Demo function call\n",
    "demo_item_counts_ex7 = {'blue fish': 44, \n",
    "                    'one fish': 47, \n",
    "                    'red fish': 100, \n",
    "                    'two fish': 74}\n",
    "demo_pair_counts_ex7 = {('blue fish', 'one fish'): 16,\n",
    "                    ('one fish', 'blue fish'): 16,\n",
    "                    ('blue fish', 'red fish'): 36,\n",
    "                    ('red fish', 'blue fish'): 36,\n",
    "                    ('blue fish', 'two fish'): 27,\n",
    "                    ('two fish', 'blue fish'): 27,\n",
    "                    ('one fish', 'red fish'): 38,\n",
    "                    ('red fish', 'one fish'): 38,\n",
    "                    ('one fish', 'two fish'): 28,\n",
    "                    ('two fish', 'one fish'): 28,\n",
    "                    ('red fish', 'two fish'): 59,\n",
    "                    ('two fish', 'red fish'): 59}\n",
    "demo_rules = create_rules_from_counts(demo_pair_counts_ex7, demo_item_counts_ex7)\n",
    "utils.print_rules(demo_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "create_rules_from_counts.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "conf(blue fish => red fish) = 0.818\n",
    "conf(one fish => red fish) = 0.809\n",
    "conf(two fish => red fish) = 0.797\n",
    "conf(blue fish => two fish) = 0.614\n",
    "conf(one fish => two fish) = 0.596\n",
    "conf(red fish => two fish) = 0.590\n",
    "conf(red fish => one fish) = 0.380\n",
    "conf(two fish => one fish) = 0.378\n",
    "conf(two fish => blue fish) = 0.365\n",
    "conf(blue fish => one fish) = 0.364\n",
    "conf(red fish => blue fish) = 0.360\n",
    "conf(one fish => blue fish) = 0.340\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for create_rules_from_counts (exercise 6). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_6",
     "locked": true,
     "points": 1,
     "solution": false
    },
    "tags": [
     "create_rules_from_counts.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.09 seconds\n",
      "memory after test: 0.04 MB\n",
      "memory peak during test: 1.39 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 6  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=create_rules_from_counts,\n",
    "              ex_name='create_rules_from_counts',\n",
    "              key=b'uB5AD-6LZ4KH6PExkCGTzp065lKUINubYeq5q9rcV00=', \n",
    "              n_iter=30)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to create_rules_from_counts did not pass the test.'\n",
    "\n",
    "\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application in our algorithm: confidence threshold\n",
    "\n",
    "The last step in our algorithm is to filter out rules with a low confidence. We want to remove any rules which do not have confidence that is at least some threshold value. The value itself should be parameterized so that users can tune it to their own application and use-case.\n",
    "\n",
    "To accomplish the task, we can create a new dictionary\n",
    "\n",
    "```python\n",
    "filtered_rules = {}\n",
    "```\n",
    "\n",
    "_Then iterate over the key/value (`rule`/`conf`) pairs in the unfiltered rules, adding any pair with confidence of at least the threshold to the new dictionary._\n",
    "\n",
    "```python\n",
    "filtered_rules.update({rule: conf})\n",
    "```  \n",
    "  \n",
    "_When the iterations are all finished, `filtered_rules` will be a dictionary containing only the rules where the confidence value is at least the threshold value._  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "filter_rules_by_conf.prompt"
    ]
   },
   "source": [
    "### Exercise 7: (1 points)\n",
    "**filter_rules_by_conf**  \n",
    "\n",
    "**Your task:** define `filter_rules_by_conf` as follows:\n",
    "\n",
    "\n",
    "Filter rules by confidence threshold.\n",
    "\n",
    "Args:\n",
    "\n",
    "- rules (dict): A dictionary mapping pairs (a, b) to confidence values.\n",
    "- threshold (float): The minimum confidence threshold.\n",
    "\n",
    "Returns:\n",
    "\n",
    "- dict: A dictionary containing only the rules with confidence >= threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "tags": [
     "filter_rules_by_conf.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf(blue fish => red fish) = 0.818\n",
      "conf(one fish => red fish) = 0.809\n",
      "conf(two fish => red fish) = 0.797\n",
      "conf(blue fish => two fish) = 0.614\n",
      "conf(one fish => two fish) = 0.596\n",
      "conf(red fish => two fish) = 0.590\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 7  \n",
    "def filter_rules_by_conf(rules, threshold):\n",
    "    filtered_rules = {}\n",
    "    for key, val in rules.items():\n",
    "        if val >= threshold:\n",
    "            filtered_rules.update({key:val})\n",
    "    return filtered_rules\n",
    "    \n",
    "### Demo function call\n",
    "demo_rules_ex7 = {('blue fish', 'one fish'): 0.36363636363636365,\n",
    "                    ('one fish', 'blue fish'): 0.3404255319148936,\n",
    "                    ('blue fish', 'red fish'): 0.8181818181818182,\n",
    "                    ('red fish', 'blue fish'): 0.36,\n",
    "                    ('blue fish', 'two fish'): 0.6136363636363636,\n",
    "                    ('two fish', 'blue fish'): 0.36486486486486486,\n",
    "                    ('one fish', 'red fish'): 0.8085106382978723,\n",
    "                    ('red fish', 'one fish'): 0.38,\n",
    "                    ('one fish', 'two fish'): 0.5957446808510638,\n",
    "                    ('two fish', 'one fish'): 0.3783783783783784,\n",
    "                    ('red fish', 'two fish'): 0.59,\n",
    "                    ('two fish', 'red fish'): 0.7972972972972973}\n",
    "demo_threshold_ex7 = 0.59\n",
    "demo_result_ex7 =filter_rules_by_conf(demo_rules_ex7, demo_threshold_ex7)\n",
    "utils.print_rules(demo_result_ex7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "filter_rules_by_conf.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "conf(blue fish => red fish) = 0.818\n",
    "conf(one fish => red fish) = 0.809\n",
    "conf(two fish => red fish) = 0.797\n",
    "conf(blue fish => two fish) = 0.614\n",
    "conf(one fish => two fish) = 0.596\n",
    "conf(red fish => two fish) = 0.590\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for filter_rules_by_conf (exercise 7). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_7",
     "locked": true,
     "points": 1,
     "solution": false
    },
    "tags": [
     "filter_rules_by_conf.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.09 seconds\n",
      "memory after test: 0.03 MB\n",
      "memory peak during test: 1.39 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 7  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=filter_rules_by_conf,\n",
    "              ex_name='filter_rules_by_conf',\n",
    "              key=b'uB5AD-6LZ4KH6PExkCGTzp065lKUINubYeq5q9rcV00=', \n",
    "              n_iter=30)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to filter_rules_by_conf did not pass the test.'\n",
    "\n",
    "\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polishing the algorithm\n",
    "\n",
    "So far we have the building blocks to implement the pairwise association mining algorithm.\n",
    "\n",
    "1. Parse \"receipts\" into sets of distinct \"items\".\n",
    "2. Process the itemsets into co-occurrence and occurrence counts.\n",
    "3. Process the counts into confidence rules.\n",
    "4. Filter the confidence rules based on a threshold.\n",
    "\n",
    "We're off to a good start, but there's still a few things to sort out...\n",
    "\n",
    "### How to make itemsets a more generic concept\n",
    "\n",
    "Unfortunately, `make_itemsets_unstructured_text` is very specific to the \"words are receipts; letters are items\" use-case. It would be nice to have another way to make item sets from different kinds of data. \n",
    "\n",
    "Let's look at some real data which someone was nice enouth to prepare for a similar exercise designed for the R programming language. (Earlier versions of this notebook linked to a now defunct blog post with details on the original.)\n",
    "\n",
    "Run the next two cells to load and explore this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "tags": [
     "make_itemsets_csv.preload_objects"
    ]
   },
   "outputs": [],
   "source": [
    "### Run Me!!!\n",
    "groceries_file = utils.load_object_from_publicdata('groceries_file')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below displays the first 10 lines from `groceries_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(groceries_file)=<class 'str'>\n",
      "Length of groceries_file: 500843 characters\n",
      "\n",
      "Content Preview:\n",
      "citrus fruit,semi-finished bread,margarine,ready soups\n",
      "tropical fruit,yogurt,coffee\n",
      "whole milk\n",
      "pip fruit,yogurt,cream cheese ,meat spreads\n",
      "other vegetables,whole milk,condensed milk,long life bakery product\n",
      "whole milk,butter,yogurt,rice,abrasive cleaner\n",
      "rolls/buns\n",
      "other vegetables,UHT-milk,rolls/buns,bottled beer,liquor (appetizer)\n",
      "pot plants\n",
      "whole milk,cereals\n"
     ]
    }
   ],
   "source": [
    "print(f'{type(groceries_file)=}')\n",
    "print(f'Length of groceries_file: {len(groceries_file)} characters')\n",
    "print('\\nContent Preview:')\n",
    "print('\\n'.join(groceries_file.splitlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is organized differently than in our unstructured text. It's in a csv-like format.\n",
    "\n",
    "- Each \"receipt\" is a line.\n",
    "- The \"items\" in each line are separated by commas (',').\n",
    "\n",
    "To parse this csv string into itemsets, we need to:\n",
    "\n",
    "- Create an empty list to store the final output.\n",
    "- Divide the data into individual lines.\n",
    "- For each line\n",
    "  - Split by commas.\n",
    "  - Create a set out of the result.\n",
    "  - Append the set to the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "make_itemsets_csv.prompt"
    ]
   },
   "source": [
    "### Exercise 8: (3 points)\n",
    "**make_itemsets_csv**  \n",
    "\n",
    "**Your task:** define `make_itemsets_csv` as follows:\n",
    "\n",
    "\n",
    "Given a CSV string where each line represents a receipt, return a list of sets corresponding to the distinct items in each receipt.\n",
    "\n",
    "Args:\n",
    "\n",
    "- csv_str (str): The input CSV string.\n",
    "    - Each line corresponds to a receipt.\n",
    "    - Items in each receipt are separated by commas.\n",
    "\n",
    "Returns:\n",
    "- list of sets: A list where each element is a set of distinct items from a receipt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "tags": [
     "make_itemsets_csv.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"milk,eggs,peanut butter,oatmeal\n",
      "butter,pancake mix,maple syrup\n",
      "dog treats,milk,milk\"\"\"\n",
      "make_itemsets_csv(demo_csv_str_ex8)=[{'peanut butter', 'oatmeal', 'eggs', 'milk'}, {'pancake mix', 'butter', 'maple syrup'}, {'dog treats', 'milk'}]\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 8  \n",
    "def make_itemsets_csv(csv_str):\n",
    "    list = []\n",
    "    csv = csv_str.split('\\n')\n",
    "    for line in csv:\n",
    "        list.append(set(line.split(',')))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return list\n",
    "\n",
    "###Create an empty list to store the final output.\n",
    "#Divide the data into individual lines.\n",
    "#For each line\n",
    "#Split by commas.\n",
    "#Create a set out of the result.\n",
    "#Append the set to the final output.\n",
    "\n",
    "### Demo function call\n",
    "demo_csv_str_ex8 = dedent('''\\\n",
    "                        milk,eggs,peanut butter,oatmeal\n",
    "                        butter,pancake mix,maple syrup\n",
    "                        dog treats,milk,milk\n",
    "                        ''').strip()\n",
    "print(f'\"\"\"{demo_csv_str_ex8}\"\"\"')\n",
    "print(f'{make_itemsets_csv(demo_csv_str_ex8)=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "make_itemsets_csv.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "\"\"\"milk,eggs,peanut butter,oatmeal\n",
    "butter,pancake mix,maple syrup\n",
    "dog treats,milk,milk\"\"\"\n",
    "make_itemsets_csv(demo_csv_str_ex8)=[{'eggs', 'milk', 'peanut butter', 'oatmeal'}, {'maple syrup', 'butter', 'pancake mix'}, {'milk', 'dog treats'}]\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for make_itemsets_csv (exercise 8). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_8",
     "locked": true,
     "points": 3,
     "solution": false
    },
    "tags": [
     "make_itemsets_csv.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.07 seconds\n",
      "memory after test: 0.03 MB\n",
      "memory peak during test: 1.38 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 8  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=make_itemsets_csv,\n",
    "              ex_name='make_itemsets_csv',\n",
    "              key=b'uB5AD-6LZ4KH6PExkCGTzp065lKUINubYeq5q9rcV00=', \n",
    "              n_iter=30)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to make_itemsets_csv did not pass the test.'\n",
    "\n",
    "\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Higher order functions\n",
    "\n",
    "Python functions are _objects_ (like strings, integers, lists, etc.). As objects, functions can be passed as arguments to other functions. You have seen this before when passing the `key` argument to `sorted` in Notebook 1; we passed a function as that argument!\n",
    "\n",
    "Let's say we need to write some function which pre-processes a string then parses it into a list of words, but we don't know what \"pre-process\" looks like. We can still do it, but the user is going to have to define the preprocessing logic on their side. Our function will stitch in the logic at runtime.\n",
    "\n",
    "Here's our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_and_parse(s, preprocessor):\n",
    "    \"\"\"\n",
    "    Preprocess the string `s` using the function `preprocessor`, then parse the result into a list.\n",
    "    \n",
    "    Args:\n",
    "    - s (str): The input string to preprocess and parse.\n",
    "    - preprocessor (function): A function that takes a string as input and returns a processed string.\n",
    "    \n",
    "    Returns:\n",
    "    - list: The parsed list from the preprocessed string.\n",
    "    \"\"\"\n",
    "    preprocessed = preprocessor(s)\n",
    "    return preprocessed.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax is simple, just call `preprocessor`. Whenever the `preprocess_and_parse` is called, it's up to the caller to supply a function that can be used the same way `preprocessor` is used. \n",
    "\n",
    "The docstring requires \"A function that takes a string as input and returns a processed string.\"\n",
    "\n",
    "Here's a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess_and_parse(demo_string, lowercase_preprocessor)=['i', 'love', 'python!', \"it's\", 'awesome.']\n",
      "\n",
      "preprocess_and_parse(demo_string, remove_vowels_preprocessor)=['Lv', 'Pythn!', \"t's\", 'wsm.']\n",
      "\n",
      "preprocess_and_parse(demo_string, identity_preprocessor)=['I', 'Love', 'Python!', \"It's\", 'Awesome.']\n",
      "\n",
      "preprocess_and_parse(demo_string, alternating_case_preprocessor)=['i', 'lOvE', 'PyThOn!', \"It's\", 'aWeSoMe.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lowercase_preprocessor(s):\n",
    "    return s.lower()\n",
    "\n",
    "def remove_vowels_preprocessor(s):\n",
    "    return ''.join([c for c in s if c.lower() not in 'aeiou'])\n",
    "\n",
    "def identity_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "def alternating_case_preprocessor(s):\n",
    "    return ''.join([c.lower() if i % 2 == 0 else c.upper() for i, c in enumerate(s)])\n",
    "\n",
    "# Demo function calls\n",
    "demo_string = \"I Love Python! It's Awesome.\"\n",
    "\n",
    "print(f'{preprocess_and_parse(demo_string, lowercase_preprocessor)=}\\n')\n",
    "print(f'{preprocess_and_parse(demo_string, remove_vowels_preprocessor)=}\\n')\n",
    "print(f'{preprocess_and_parse(demo_string, identity_preprocessor)=}\\n')\n",
    "print(f'{preprocess_and_parse(demo_string, alternating_case_preprocessor)=}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty powerful concept which allows us to write re-usable functions which can take other functions as arguments to adapt to various use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "\n",
    "It's time to write one function to parse the input data into confidence rules from end to end. These are the high-level requirements.\n",
    "\n",
    "1. Parse \"receipts\" into sets of distinct \"items\".\n",
    "    - The user should be able to define how this is done.\n",
    "2. Process the itemsets into co-occurrence and occurrence counts.\n",
    "3. Process the counts into confidence rules.\n",
    "4. Filter the confidence rules based on a threshold.\n",
    "5. Filter the confidence rules based on a minimum occurrence count.\n",
    "\n",
    "There's two new bits here:\n",
    "\n",
    "- The _user_ defines how the source data gets processed into itemsets.\n",
    "    - We will implement this by taking a _function as an argument_.\n",
    "- There is an additional filter.\n",
    "    - We will have to make another pass at the rules and cross-reference the occurrence counts to decide which rules to keep under this filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "create_rules_from_source.prompt"
    ]
   },
   "source": [
    "### Exercise 9: (4 points)\n",
    "**create_rules_from_source**  \n",
    "\n",
    "**Your task:** define `create_rules_from_source` as follows:\n",
    "\n",
    "\n",
    "Create association rules from a data source using the provided itemset maker function, filtering by confidence threshold and minimum item count.\n",
    "\n",
    "Args:\n",
    "\n",
    "- source: The data source (e.g., unstructured text or CSV string).\n",
    "- itemset_maker (function): A function that takes the source as input and returns a list of itemsets (e.g. make_itemsets_csv, make_itemsets_unstructured_text).\n",
    "- conf_threshold (float): The minimum confidence threshold for filtering rules.\n",
    "- min_count (int): The minimum count for the antecedent item in the rules.\n",
    "    - I.e., `rules[(a, b)]` is only included in the output if `a` appears in at least `min_count` itemsets.\n",
    "\n",
    "Returns:\n",
    "\n",
    "- dict: A dictionary mapping pairs (a, b) to their confidence values, filtered by the specified thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- The demos below will not work if you have not correctly defined `make_itemsets_unstructured_text` or `make_itemsets_csv`. \n",
    "- The test cell uses entirely new functions as `itemset_maker` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "tags": [
     "create_rules_from_source.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: `latin_text`; Itemset Maker: `make_itemsets_unstructured_text`; Confidence Threshold: 0.75; Min Count: 0\n",
      "conf(q => u) = 1.000\n",
      "conf(v => t) = 0.818\n",
      "conf(r => e) = 0.800\n",
      "conf(v => e) = 0.773\n",
      "conf(b => i) = 0.750\n",
      "\n",
      "Source: `groceries_file`; Itemset Maker: `make_itemsets_csv`; Confidence Threshold: 0.5; Min Count: 10\n",
      "conf(honey => whole milk) = 0.733\n",
      "conf(frozen fruits => other vegetables) = 0.667\n",
      "conf(cereals => whole milk) = 0.643\n",
      "conf(rice => whole milk) = 0.613\n",
      "conf(rubbing alcohol => whole milk) = 0.600\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 9  \n",
    "def create_rules_from_source(source, itemset_maker, conf_threshold, min_count):\n",
    "    itemsets = itemset_maker(source)\n",
    "    pair_counts = defaultdict(int)\n",
    "    item_counts = defaultdict(int)\n",
    "    for itemset in  itemsets:\n",
    "        update_pair_counts(pair_counts, itemset)\n",
    "        update_item_counts(item_counts, itemset)\n",
    "    \n",
    "\n",
    "        rules = {} # (item_a, item_b) -> conf (item_a => item_b)\n",
    "    \n",
    "    for (a, b) in pair_counts:\n",
    "        if item_counts[a] >= min_count:\n",
    "            conf = pair_counts[(a, b)] / item_counts[a]\n",
    "            if conf >= conf_threshold:\n",
    "                rules[(a,b)] = conf\n",
    "                  \n",
    "    return rules\n",
    "    \n",
    "        \n",
    "    \n",
    "### Demo function call\n",
    "latin_rules = create_rules_from_source(latin_text, make_itemsets_unstructured_text, 0.75, 15)\n",
    "grocery_rules = create_rules_from_source(groceries_file, make_itemsets_csv, 0.6, 10)\n",
    "print('Source: `latin_text`; Itemset Maker: `make_itemsets_unstructured_text`; Confidence Threshold: 0.75; Min Count: 0')\n",
    "utils.print_rules(latin_rules)\n",
    "print()\n",
    "print('Source: `groceries_file`; Itemset Maker: `make_itemsets_csv`; Confidence Threshold: 0.5; Min Count: 10')\n",
    "utils.print_rules(grocery_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "create_rules_from_source.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "Source: `latin_text`; Itemset Maker: `make_itemsets_unstructured_text`; Confidence Threshold: 0.75; Min Count: 0\n",
    "conf(q => u) = 1.000\n",
    "conf(v => t) = 0.818\n",
    "conf(r => e) = 0.800\n",
    "conf(v => e) = 0.773\n",
    "conf(b => i) = 0.750\n",
    "\n",
    "Source: `groceries_file`; Itemset Maker: `make_itemsets_csv`; Confidence Threshold: 0.5; Min Count: 10\n",
    "conf(honey => whole milk) = 0.733\n",
    "conf(frozen fruits => other vegetables) = 0.667\n",
    "conf(cereals => whole milk) = 0.643\n",
    "conf(rice => whole milk) = 0.613\n",
    "conf(rubbing alcohol => whole milk) = 0.600\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for create_rules_from_source (exercise 9). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_9",
     "locked": true,
     "points": 4,
     "solution": false
    },
    "tags": [
     "create_rules_from_source.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 1.50 seconds\n",
      "memory after test: 0.42 MB\n",
      "memory peak during test: 30.23 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 9  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=create_rules_from_source,\n",
    "              ex_name='create_rules_from_source',\n",
    "              key=b'uB5AD-6LZ4KH6PExkCGTzp065lKUINubYeq5q9rcV00=', \n",
    "              n_iter=30)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to create_rules_from_source did not pass the test.'\n",
    "\n",
    "\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "Our goal was to parse structured data into association rules. These are useful in their own right, offering insights into which items should be offered far apart in a store to encourage customers to traverse the whole floor spare. We can use the rules in different ways.\n",
    "\n",
    "Say we want to look at association rules for all the stores in a region to predict which associations we can count on holding true for a new store.\n",
    "\n",
    "To do that we would need a collection of rules, and we simply take the set intersection of all the keys (recall rules are modeled as dictionaries mapping ordered item pairs to confidence values). There is one edge case - what if the list of rules is empty? One option is to return `None`. This distinguishes the \"empty rules list\" case from the \"no common rules\" case, and is what we're going to implement. (There are other options, like raising an error, which is covered in a later notebook.)\n",
    "\n",
    "To handle this edge case, we can use this pattern:\n",
    "\n",
    "- Initialize variable for collecting the intersection as `None`.\n",
    "  - i.e. `result = None`.\n",
    "- Iterate over every \"rules\" dictionary in the rules_list.\n",
    "  - If `result is None`, set result to the current `rules` keys.\n",
    "  - Otherwise, update `result` to be the intersection of `result` and the current `rules` keys.\n",
    "\n",
    "The cell below loads the English translation of the Lorem Ipsum, which will be used to derive rules for comparison with Latin rules. Let's see what letter pairs are common to both languages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "tags": [
     "common_rules.preload_objects"
    ]
   },
   "outputs": [],
   "source": [
    "### Run Me!!!\n",
    "english_text = utils.load_object_from_publicdata('english_text')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "common_rules.prompt"
    ]
   },
   "source": [
    "### Exercise 10: (2 points)\n",
    "**common_rules**  \n",
    "\n",
    "**Your task:** define `common_rules` as follows:\n",
    "\n",
    "\n",
    "Given a list of rules dictionaries, return the set of rules (keys) that are common to all dictionaries.\n",
    "\n",
    "Args:\n",
    "\n",
    "- rules_list (list of dict): A list where each element is a dictionary mapping pairs (a, b) to confidence values.\n",
    "\n",
    "Returns:\n",
    "\n",
    "- set or None: A set of pairs (a, b) that are present as keys in all dictionaries in the input list. If the input list is empty, return None.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "tags": [
     "common_rules.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latin rules:\n",
      "conf(q => u) = 1.000\n",
      "conf(x => e) = 1.000\n",
      "conf(h => i) = 0.833\n",
      "conf(x => i) = 0.833\n",
      "conf(v => t) = 0.818\n",
      "conf(r => e) = 0.800\n",
      "\n",
      "English rules:\n",
      "conf(x => e) = 1.000\n",
      "conf(j => e) = 1.000\n",
      "conf(q => u) = 1.000\n",
      "conf(q => e) = 1.000\n",
      "\n",
      "Common rules:\n",
      "x => e\n",
      "q => u\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 10  \n",
    "def common_rules(rules_list):\n",
    "    if not rules_list:\n",
    "        return None\n",
    "    common_rules = set(rules_list[0].keys())\n",
    "    for dict in rules_list[1:]:\n",
    "        common_rules.intersection_update(dict.keys())\n",
    "    return common_rules\n",
    "        \n",
    "    \n",
    "\n",
    "### Demo function call\n",
    "latin_rules = create_rules_from_source(latin_text, make_itemsets_unstructured_text, 0.8, 3)\n",
    "english_rules = create_rules_from_source(english_text, make_itemsets_unstructured_text, 0.8, 3)\n",
    "print('Latin rules:')\n",
    "utils.print_rules(latin_rules)\n",
    "print('\\nEnglish rules:')\n",
    "utils.print_rules(english_rules)\n",
    "print('\\nCommon rules:')\n",
    "demo_result_ex10 = common_rules([latin_rules, english_rules])\n",
    "utils.print_rules(demo_result_ex10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "common_rules.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "Latin rules:\n",
    "conf(q => u) = 1.000\n",
    "conf(x => e) = 1.000\n",
    "conf(h => i) = 0.833\n",
    "conf(x => i) = 0.833\n",
    "conf(v => t) = 0.818\n",
    "conf(r => e) = 0.800\n",
    "\n",
    "English rules:\n",
    "conf(x => e) = 1.000\n",
    "conf(j => e) = 1.000\n",
    "conf(q => e) = 1.000\n",
    "conf(q => u) = 1.000\n",
    "\n",
    "Common rules:\n",
    "x => e\n",
    "q => u\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for common_rules (exercise 10). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_10",
     "locked": true,
     "points": 2,
     "solution": false
    },
    "tags": [
     "common_rules.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 1.10 MB\n",
      "Test duration: 0.09 seconds\n",
      "memory after test: 1.40 MB\n",
      "memory peak during test: 30.23 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 10  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=common_rules,\n",
    "              ex_name='common_rules',\n",
    "              key=b'uB5AD-6LZ4KH6PExkCGTzp065lKUINubYeq5q9rcV00=', \n",
    "              n_iter=32)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to common_rules did not pass the test.'\n",
    "\n",
    "\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
