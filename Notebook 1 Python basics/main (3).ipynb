{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python review: More exercises\n",
    "\n",
    "This notebook continues the review of Python basics. A key concept is that of a _nested_ data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "tags": [
     "main.global_imports"
    ]
   },
   "outputs": [],
   "source": [
    "### Global imports\n",
    "import dill\n",
    "from cse6040_devkit import plugins, utils\n",
    "from cse6040_devkit.training_wheels import run_with_timeout, suppress_stdout\n",
    "import tracemalloc\n",
    "from time import time\n",
    "import re \n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following dataset of exam grades, organized as a 2-D table and stored in Python as a \"list of lists\" under the variable name, `grades`. You will use this 2-D table throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grades = [\n",
    "    # First line is descriptive header. Subsequent lines hold data\n",
    "    ['Student', 'Exam 1', 'Exam 2', 'Exam 3'],\n",
    "    ['Thorny', '100', '90', '80'],\n",
    "    ['Mac', '88', '99', '111'],\n",
    "    ['Farva', '45', '56', '67'],\n",
    "    ['Rabbit', '59', '61', '67'],\n",
    "    ['Ursula', '73', '79', '83'],\n",
    "    ['Foster', '89', '97', '101']\n",
    "]\n",
    "\n",
    "grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "get_students.prompt"
    ]
   },
   "source": [
    "### Exercise 0: (1 points)\n",
    "**get_students**  \n",
    "\n",
    "**Your task:** define `get_students` as follows:\n",
    "\n",
    "\n",
    "Write a function that returns the students names from \"top to bottom\".\n",
    "\n",
    "**Inputs**:\n",
    "- `grades` (list): a nested list of grades\n",
    "\n",
    "**Return**:\n",
    "- `students` (list): a list representing the list of students names as they appear from \"top to bottom\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "tags": [
     "get_students.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result=['Thorny', 'Mac', 'Farva', 'Rabbit', 'Ursula', 'Foster']\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 0  \n",
    "def get_students(grades: list) -> list:\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    students = []\n",
    "    for row in grades[1:]:\n",
    "        students.append(row[0])\n",
    "    return students\n",
    "            \n",
    "\n",
    "### Demo function call\n",
    "grades = [\n",
    "# First line is descriptive header. Subsequent lines hold data\n",
    "['Student', 'Exam 1', 'Exam 2', 'Exam 3'],\n",
    "['Thorny', '100', '90', '80'],\n",
    "['Mac', '88', '99', '111'],\n",
    "['Farva', '45', '56', '67'],\n",
    "['Rabbit', '59', '61', '67'],\n",
    "['Ursula', '73', '79', '83'],\n",
    "['Foster', '89', '97', '101']\n",
    "]\n",
    "print(f'result={get_students(grades)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "get_students.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "result=['Thorny', 'Mac', 'Farva', 'Rabbit', 'Ursula', 'Foster']\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for get_students (exercise 0). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_0",
     "locked": true,
     "points": 1,
     "solution": false
    },
    "tags": [
     "get_students.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_students' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3b06b25c17fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Execute test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m passed, test_case_vars, e = execute_tests(func=get_students,\n\u001b[0m\u001b[1;32m     25\u001b[0m               \u001b[0mex_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'get_students'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m               \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mb'8VGfIctvRdgA0ZvvNEVbMI2yu6UDz3sdBSZX59VpBSo='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_students' is not defined"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 0  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=get_students,\n",
    "              ex_name='get_students',\n",
    "              key=b'8VGfIctvRdgA0ZvvNEVbMI2yu6UDz3sdBSZX59VpBSo=', \n",
    "              n_iter=50)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to get_students did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "get_assignments.prompt"
    ]
   },
   "source": [
    "### Exercise 1: (1 points)\n",
    "**get_assignments**  \n",
    "\n",
    "**Your task:** define `get_assignments` as follows:\n",
    "\n",
    "\n",
    "Write a function that returns the assignment names.\n",
    "\n",
    "**Inputs**:\n",
    "- `grades` (list): a nested list of grades\n",
    "\n",
    "**Return**:\n",
    "- `assignments` (list): a list representing the assignment names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "tags": [
     "get_assignments.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result=['Exam 1', 'Exam 2', 'Exam 3']\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 1  \n",
    "def get_assignments(grades: list) -> list:\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    assignments = []\n",
    "    for row in grades[0][1:]:\n",
    "        assignments.append(row)\n",
    "    return assignments\n",
    "    \n",
    "### Demo function call\n",
    "grades = [\n",
    "# First line is descriptive header. Subsequent lines hold data\n",
    "['Student', 'Exam 1', 'Exam 2', 'Exam 3'],\n",
    "['Thorny', '100', '90', '80'],\n",
    "['Mac', '88', '99', '111'],\n",
    "['Farva', '45', '56', '67'],\n",
    "['Rabbit', '59', '61', '67'],\n",
    "['Ursula', '73', '79', '83'],\n",
    "['Foster', '89', '97', '101']\n",
    "]\n",
    "print(f'result={get_assignments(grades)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "get_assignments.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "result=['Exam 1', 'Exam 2', 'Exam 3']\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for get_assignments (exercise 1). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_1",
     "locked": true,
     "points": 1,
     "solution": false
    },
    "tags": [
     "get_assignments.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.12 MB\n",
      "Test duration: 0.14 seconds\n",
      "memory after test: 3.23 MB\n",
      "memory peak during test: 4.56 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 1  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=get_assignments,\n",
    "              ex_name='get_assignments',\n",
    "              key=b'8VGfIctvRdgA0ZvvNEVbMI2yu6UDz3sdBSZX59VpBSo=', \n",
    "              n_iter=50)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to get_assignments did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "build_grade_lists.prompt"
    ]
   },
   "source": [
    "### Exercise 2: (1 points)\n",
    "**build_grade_lists**  \n",
    "\n",
    "**Your task:** define `build_grade_lists` as follows:\n",
    "\n",
    "\n",
    "Write a function that returns a dictionary of scores for each student.\n",
    "\n",
    "**Inputs**:\n",
    "- `grades` (list): a nested list of grades\n",
    "\n",
    "**Return**:\n",
    "- `grade_lists` (dict): \n",
    "  - key is a student\n",
    "  - value is a list of scores with the scores converted to integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "tags": [
     "build_grade_lists.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result={'Thorny': [100, 90, 80], 'Mac': [88, 99, 111], 'Farva': [45, 56, 67], 'Rabbit': [59, 61, 67], 'Ursula': [73, 79, 83], 'Foster': [89, 97, 101]}\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 2  \n",
    "def build_grade_lists(grades: list) -> dict:\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    names = []\n",
    "    grade = []\n",
    "    for row in grades[1:]:\n",
    "        names.append(row[0])\n",
    "        grade.append(row[1:])\n",
    "  \n",
    "    grade_int = []\n",
    "    for sublist in grade:\n",
    "        student_grade = []\n",
    "        for str in sublist:\n",
    "            student_grade.append(int(str))\n",
    "            \n",
    "        grade_int.append(student_grade)\n",
    "            \n",
    "    grade_lists = dict(zip(names, grade_int))\n",
    "    return grade_lists\n",
    "\n",
    "    \n",
    "### Demo function call\n",
    "grades = [\n",
    "# First line is descriptive header. Subsequent lines hold data\n",
    "['Student', 'Exam 1', 'Exam 2', 'Exam 3'],\n",
    "['Thorny', '100', '90', '80'],\n",
    "['Mac', '88', '99', '111'],\n",
    "['Farva', '45', '56', '67'],\n",
    "['Rabbit', '59', '61', '67'],\n",
    "['Ursula', '73', '79', '83'],\n",
    "['Foster', '89', '97', '101']\n",
    "]\n",
    "print(f'result={build_grade_lists(grades)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "build_grade_lists.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "result={'Thorny': [100, 90, 80], 'Mac': [88, 99, 111], 'Farva': [45, 56, 67], 'Rabbit': [59, 61, 67], 'Ursula': [73, 79, 83], 'Foster': [89, 97, 101]}\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for build_grade_lists (exercise 2). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_2",
     "locked": true,
     "points": 1,
     "solution": false
    },
    "tags": [
     "build_grade_lists.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.15 seconds\n",
      "memory after test: 0.05 MB\n",
      "memory peak during test: 1.43 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 2  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=build_grade_lists,\n",
    "              ex_name='build_grade_lists',\n",
    "              key=b'8VGfIctvRdgA0ZvvNEVbMI2yu6UDz3sdBSZX59VpBSo=', \n",
    "              n_iter=50)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to build_grade_lists did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "build_grade_dicts.prompt"
    ]
   },
   "source": [
    "### Exercise 3: (2 points)\n",
    "**build_grade_dicts**  \n",
    "\n",
    "**Your task:** define `build_grade_dicts` as follows:\n",
    "\n",
    "\n",
    "Write a function that returns a dictionary of scores for each student.\n",
    "\n",
    "**Inputs**:\n",
    "- `grades` (list): a nested list of grades\n",
    "\n",
    "**Return**:\n",
    "- `grade_dicts` (dict): a nested dictionary \n",
    "  - key is a student\n",
    "  - value is another dictionary\n",
    "    - key is the assignment\n",
    "    - value is the score as an integer\n",
    "\n",
    "**Hint**: \n",
    "- You may find earlier exercises useful for completing this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "tags": [
     "build_grade_dicts.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result={'Thorny': {'Exam 1': 100, 'Exam 2': 90, 'Exam 3': 80}, 'Mac': {'Exam 1': 88, 'Exam 2': 99, 'Exam 3': 111}, 'Farva': {'Exam 1': 45, 'Exam 2': 56, 'Exam 3': 67}, 'Rabbit': {'Exam 1': 59, 'Exam 2': 61, 'Exam 3': 67}, 'Ursula': {'Exam 1': 73, 'Exam 2': 79, 'Exam 3': 83}, 'Foster': {'Exam 1': 89, 'Exam 2': 97, 'Exam 3': 101}}\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 3  \n",
    "def build_grade_dicts(grades: list) -> dict:\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    \n",
    "    grade_dicts = {}\n",
    "    exams = grades[0][1:]\n",
    "   \n",
    "    \n",
    "    for row in grades[1:]:\n",
    "        names = row[0]\n",
    "        score = row[1:]\n",
    "        \n",
    "        score_int = []\n",
    "        for s in score:\n",
    "            score_int.append(int(s))\n",
    "            \n",
    "        exam_score = dict(zip(exams, score_int))\n",
    "        grade_dicts[names] = exam_score\n",
    "    \n",
    "    \n",
    "    return grade_dicts\n",
    "\n",
    "### Demo function call\n",
    "grades = [\n",
    "# First line is descriptive header. Subsequent lines hold data\n",
    "['Student', 'Exam 1', 'Exam 2', 'Exam 3'],\n",
    "['Thorny', '100', '90', '80'],\n",
    "['Mac', '88', '99', '111'],\n",
    "['Farva', '45', '56', '67'],\n",
    "['Rabbit', '59', '61', '67'],\n",
    "['Ursula', '73', '79', '83'],\n",
    "['Foster', '89', '97', '101']\n",
    "]\n",
    "print(f'result={build_grade_dicts(grades)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "build_grade_dicts.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "result={'Thorny': {'Exam 1': 100, 'Exam 2': 90, 'Exam 3': 80}, 'Mac': {'Exam 1': 88, 'Exam 2': 99, 'Exam 3': 111}, 'Farva': {'Exam 1': 45, 'Exam 2': 56, 'Exam 3': 67}, 'Rabbit': {'Exam 1': 59, 'Exam 2': 61, 'Exam 3': 67}, 'Ursula': {'Exam 1': 73, 'Exam 2': 79, 'Exam 3': 83}, 'Foster': {'Exam 1': 89, 'Exam 2': 97, 'Exam 3': 101}}\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for build_grade_dicts (exercise 3). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_3",
     "locked": true,
     "points": 2,
     "solution": false
    },
    "tags": [
     "build_grade_dicts.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.24 seconds\n",
      "memory after test: 3.16 MB\n",
      "memory peak during test: 4.57 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 3  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=build_grade_dicts,\n",
    "              ex_name='build_grade_dicts',\n",
    "              key=b'8VGfIctvRdgA0ZvvNEVbMI2yu6UDz3sdBSZX59VpBSo=', \n",
    "              n_iter=50)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to build_grade_dicts did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "build_avg_by_student.prompt"
    ]
   },
   "source": [
    "### Exercise 4: (1 points)\n",
    "**build_avg_by_student**  \n",
    "\n",
    "**Your task:** define `build_avg_by_student` as follows:\n",
    "\n",
    "\n",
    "Write a function that returns a dictionary of average scores for each student.\n",
    "\n",
    "**Inputs**:\n",
    "- `grades` (list): a nested list of grades\n",
    "\n",
    "**Return**:\n",
    "- `avg_grades` (dict): a dictionary \n",
    "  - key is a student\n",
    "  - value is an average grade for that student rounded to 3 decimal places\n",
    "\n",
    "**Hint**: \n",
    "- You may find earlier exercises useful for completing this exercise.\n",
    "- The [statistics](https://docs.python.org/3.5/library/statistics.html) module has at least one helpful function.\n",
    "- The [round](https://docs.python.org/3/library/functions.html#round) function will be helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "tags": [
     "build_avg_by_student.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result={'Thorny': 90.0, 'Mac': 99.333, 'Farva': 56.0, 'Rabbit': 62.333, 'Ursula': 78.333, 'Foster': 95.667}\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 4  \n",
    "def build_avg_by_student(grades: list) -> dict:\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    avg_grades = {}\n",
    "    for row in grades[1:]:\n",
    "        student_names = row[0]\n",
    "        scores = row[1:]\n",
    "        scores_int = []\n",
    "        for s in scores:\n",
    "            scores_int.append(int(s))\n",
    "        score_ave = round(sum(scores_int) / len(scores_int),3) \n",
    "    \n",
    "        avg_grades[student_names] = score_ave\n",
    "    \n",
    "    return avg_grades\n",
    "\n",
    "### Demo function call\n",
    "grades = [\n",
    "# First line is descriptive header. Subsequent lines hold data\n",
    "['Student', 'Exam 1', 'Exam 2', 'Exam 3'],\n",
    "['Thorny', '100', '90', '80'],\n",
    "['Mac', '88', '99', '111'],\n",
    "['Farva', '45', '56', '67'],\n",
    "['Rabbit', '59', '61', '67'],\n",
    "['Ursula', '73', '79', '83'],\n",
    "['Foster', '89', '97', '101']\n",
    "]\n",
    "print(f'result={build_avg_by_student(grades)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "build_avg_by_student.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "result={'Thorny': 90, 'Mac': 99.333, 'Farva': 56, 'Rabbit': 62.333, 'Ursula': 78.333, 'Foster': 95.667}\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for build_avg_by_student (exercise 4). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_4",
     "locked": true,
     "points": 1,
     "solution": false
    },
    "tags": [
     "build_avg_by_student.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.09 seconds\n",
      "memory after test: 0.04 MB\n",
      "memory peak during test: 1.46 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 4  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=build_avg_by_student,\n",
    "              ex_name='build_avg_by_student',\n",
    "              key=b'8VGfIctvRdgA0ZvvNEVbMI2yu6UDz3sdBSZX59VpBSo=', \n",
    "              n_iter=50)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to build_avg_by_student did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "build_grade_by_asn.prompt"
    ]
   },
   "source": [
    "### Exercise 5: (2 points)\n",
    "**build_grade_by_asn**  \n",
    "\n",
    "**Your task:** define `build_grade_by_asn` as follows:\n",
    "\n",
    "\n",
    "Write a function that returns a dictionary of scores for each assignment.\n",
    "\n",
    "**Inputs**:\n",
    "- `grades` (list): a nested list of grades\n",
    "\n",
    "**Return**:\n",
    "- `grades_by_assignment` (dict): a dictionary \n",
    "  - key is an assignment\n",
    "  - value is a list of scores with the scores converted to integers\n",
    "\n",
    "**Hint**: \n",
    "- You may find earlier exercises useful for completing this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "tags": [
     "build_grade_by_asn.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result={'Exam 1': [100, 88, 45, 59, 73, 89], 'Exam 2': [90, 99, 56, 61, 79, 97], 'Exam 3': [80, 111, 67, 67, 83, 101]}\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 5  \n",
    "def build_grade_by_asn(grades: list) -> dict:\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    grades_by_assignment = {}\n",
    "    \n",
    "    exams = grades[0][1:]\n",
    "    for exam in exams:\n",
    "        grades_by_assignment[exam] =[]\n",
    "        \n",
    "\n",
    "    for row in grades[1:]:\n",
    "        scores = row[1:]\n",
    "       \n",
    "        for i in range(len(exams)):\n",
    "            exam_name = exams[i]\n",
    "            score_int = int(scores[i])\n",
    "            grades_by_assignment[exam_name].append(score_int)         \n",
    "                       \n",
    "                               \n",
    "    return grades_by_assignment\n",
    "            \n",
    "        \n",
    "### Demo function call\n",
    "grades = [\n",
    "# First line is descriptive header. Subsequent lines hold data\n",
    "['Student', 'Exam 1', 'Exam 2', 'Exam 3'],\n",
    "['Thorny', '100', '90', '80'],\n",
    "['Mac', '88', '99', '111'],\n",
    "['Farva', '45', '56', '67'],\n",
    "['Rabbit', '59', '61', '67'],\n",
    "['Ursula', '73', '79', '83'],\n",
    "['Foster', '89', '97', '101']\n",
    "]\n",
    "print(f'result={build_grade_by_asn(grades)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "build_grade_by_asn.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "result={'Exam 1': [100, 88, 45, 59, 73, 89], 'Exam 2': [90, 99, 56, 61, 79, 97], 'Exam 3': [80, 111, 67, 67, 83, 101]}\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for build_grade_by_asn (exercise 5). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_5",
     "locked": true,
     "points": 2,
     "solution": false
    },
    "tags": [
     "build_grade_by_asn.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.28 seconds\n",
      "memory after test: 3.18 MB\n",
      "memory peak during test: 4.59 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 5  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=build_grade_by_asn,\n",
    "              ex_name='build_grade_by_asn',\n",
    "              key=b'8VGfIctvRdgA0ZvvNEVbMI2yu6UDz3sdBSZX59VpBSo=', \n",
    "              n_iter=50)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to build_grade_by_asn did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "build_avg_by_asn.prompt"
    ]
   },
   "source": [
    "### Exercise 6: (1 points)\n",
    "**build_avg_by_asn**  \n",
    "\n",
    "**Your task:** define `build_avg_by_asn` as follows:\n",
    "\n",
    "\n",
    "Write a function that returns a dictionary of average scores for each assignment.\n",
    "\n",
    "**Inputs**:\n",
    "- `grades` (list): a nested list of grades\n",
    "\n",
    "**Return**:\n",
    "- `avg_assignment` (dict): a dictionary \n",
    "  - key is an assignment\n",
    "  - value is an average grade for that assignment rounded to 3 decimal places\n",
    "\n",
    "**Hint**: \n",
    "- You may find earlier exercises useful for completing this exercise.\n",
    "- The [statistics](https://docs.python.org/3.5/library/statistics.html) module has at least one helpful function.\n",
    "- The [round](https://docs.python.org/3/library/functions.html#round) function will be helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "tags": [
     "build_avg_by_asn.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result={'Exam 1': 75.667, 'Exam 2': 80.333, 'Exam 3': 84.833}\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 6  \n",
    "def build_avg_by_asn(grades: list) -> dict:\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    avg_assignment = {}\n",
    "   \n",
    "    exams = grades[0][1:]\n",
    "    for exam in exams:\n",
    "        avg_assignment[exam] = []\n",
    "    \n",
    "    for row in grades[1:]:\n",
    "        scores = row[1:]\n",
    "        \n",
    "        for i in range(len(exams)):\n",
    "            exam_name = exams[i]\n",
    "            score = int(scores[i])\n",
    "            avg_assignment[exam_name].append(score)\n",
    "            \n",
    "    for exam_name in exams:\n",
    "        all_scores = avg_assignment[exam_name]\n",
    "        av_score = round(sum(all_scores) / len(all_scores), 3)\n",
    "        avg_assignment[exam_name] = av_score\n",
    "        \n",
    "        \n",
    "                        \n",
    "    return avg_assignment\n",
    "        \n",
    "\n",
    "### Demo function call\n",
    "grades = [\n",
    "# First line is descriptive header. Subsequent lines hold data\n",
    "['Student', 'Exam 1', 'Exam 2', 'Exam 3'],\n",
    "['Thorny', '100', '90', '80'],\n",
    "['Mac', '88', '99', '111'],\n",
    "['Farva', '45', '56', '67'],\n",
    "['Rabbit', '59', '61', '67'],\n",
    "['Ursula', '73', '79', '83'],\n",
    "['Foster', '89', '97', '101']\n",
    "]\n",
    "print(f'result={build_avg_by_asn(grades)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "build_avg_by_asn.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "result={'Exam 1': 75.667, 'Exam 2': 80.333, 'Exam 3': 84.833}\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for build_avg_by_asn (exercise 6). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_6",
     "locked": true,
     "points": 1,
     "solution": false
    },
    "tags": [
     "build_avg_by_asn.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.08 seconds\n",
      "memory after test: 0.03 MB\n",
      "memory peak during test: 1.43 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 6  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=build_avg_by_asn,\n",
    "              ex_name='build_avg_by_asn',\n",
    "              key=b'8VGfIctvRdgA0ZvvNEVbMI2yu6UDz3sdBSZX59VpBSo=', \n",
    "              n_iter=50)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to build_avg_by_asn did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "get_ranked_students.prompt"
    ]
   },
   "source": [
    "### Exercise 7: (2 points)\n",
    "**get_ranked_students**  \n",
    "\n",
    "**Your task:** define `get_ranked_students` as follows:\n",
    "\n",
    "\n",
    "Write a function that returns a list of sorted students.\n",
    "\n",
    "**Inputs**:\n",
    "- `grades` (list): a nested list of grades\n",
    "\n",
    "**Return**:\n",
    "- `ranked_students` (list): a list containing students ordered by: \n",
    "  - average score descending order\n",
    "  - break any ties by name in ascending alphabetical order \n",
    "\n",
    "**Hint**: \n",
    "- You may find earlier exercises useful for completing this exercise.\n",
    "- `ranked_students[0]` would be the student with the highest average exam score while `ranked_students[-1]` would have the lowest average exam score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "tags": [
     "get_ranked_students.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result=['Abigail', 'Mac', 'Foster', 'Thorny', 'Ursula', 'Rabbit', 'Farva']\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 7  \n",
    "def get_ranked_students(grades: list) -> list:\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    ranked_students = []\n",
    "    avg_grades = {}\n",
    "    for row in grades[1:]:\n",
    "        student_names = row[0]\n",
    "        scores = row[1:]\n",
    "        scores_int = []\n",
    "        for s in scores:\n",
    "            scores_int.append(int(s))\n",
    "        score_ave = round(sum(scores_int) / len(scores_int),3) \n",
    "    \n",
    "        avg_grades[student_names] = score_ave\n",
    "        \n",
    "     #convert  avg_grades dict to a list\n",
    "    student_list = list(avg_grades.items())\n",
    "    #sort the list \n",
    "    student_list.sort(key=lambda x: (-x[1], x[0]))\n",
    "    \n",
    "    for student in student_list:\n",
    "         ranked_students.append(student[0])\n",
    "\n",
    "    \n",
    "    return ranked_students\n",
    "    \n",
    "\n",
    "### Demo function call\n",
    "grades = [\n",
    "# First line is descriptive header. Subsequent lines hold data\n",
    "['Student', 'Exam 1', 'Exam 2', 'Exam 3'],\n",
    "['Thorny', '100', '90', '80'],\n",
    "['Mac', '88', '99', '111'],\n",
    "['Farva', '45', '56', '67'],\n",
    "['Rabbit', '59', '61', '67'],\n",
    "['Ursula', '73', '79', '83'],\n",
    "['Foster', '89', '97', '101'],\n",
    "['Abigail', '99', '99', '100']\n",
    "]\n",
    "print(f'result={get_ranked_students(grades)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "get_ranked_students.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "result=['Abigail', 'Mac', 'Foster', 'Thorny', 'Ursula', 'Rabbit', 'Farva']\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for get_ranked_students (exercise 7). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_7",
     "locked": true,
     "points": 2,
     "solution": false
    },
    "tags": [
     "get_ranked_students.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.06 seconds\n",
      "memory after test: 0.03 MB\n",
      "memory peak during test: 1.45 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 7  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=get_ranked_students,\n",
    "              ex_name='get_ranked_students',\n",
    "              key=b'8VGfIctvRdgA0ZvvNEVbMI2yu6UDz3sdBSZX59VpBSo=', \n",
    "              n_iter=50)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to get_ranked_students did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIN!\n",
    "You've reached the end of this part. Don't forget to restart and run all cells again to make sure it's all working when run in sequence; and make sure your work passes the submission process. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
