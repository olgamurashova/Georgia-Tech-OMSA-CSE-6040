{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Floating-point arithmetic\n",
    "\n",
    "As a data analyst, you will be concerned primarily with _numerical programs_ in which the bulk of the computational work involves floating-point computation. This notebook guides you through some of the most fundamental concepts in how computers store real numbers, so you can be smarter about your number crunching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "tags": [
     "main.global_imports"
    ]
   },
   "outputs": [],
   "source": [
    "### Global imports\n",
    "import dill\n",
    "from cse6040_devkit import plugins, utils\n",
    "from cse6040_devkit.training_wheels import run_with_timeout, suppress_stdout\n",
    "import tracemalloc\n",
    "from time import time\n",
    "from decimal import Decimal\n",
    "from pprint import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WYSInnWYG, or \"what you see is not necessarily what you get.\"\n",
    "\n",
    "One important consequence of a binary format is that when you print values in base ten, what you see may not be what you get! For instance, try running the code below.\n",
    "\n",
    "> This code invokes Python's [`decimal`](https://docs.python.org/3/library/decimal.html) package, which implements base-10 floating-point arithmetic in software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002\n",
      "1.0000000000000002220446049250313080847263336181640625\n",
      "5.551115123125782702118158340E-18\n"
     ]
    }
   ],
   "source": [
    "x = 1.0 + 2.0**(-52)\n",
    "\n",
    "print(x)\n",
    "print(Decimal(x)) # What does this do?\n",
    "\n",
    "print(Decimal(0.1) - Decimal('0.1')) # Why does the output appear as it does?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Aside: If you ever need true decimal storage with no loss of precision (e.g., an accounting application), turn to the `decimal` package. Just be warned it might be slower. See the following experiment for a practical demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Native arithmetic:\n",
      "221 ms ± 3.13 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "Decimal package:\n",
      "645 ms ± 9.17 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "NUM_TRIALS = 2500000\n",
    "\n",
    "print(\"Native arithmetic:\")\n",
    "A_native = [random() for _ in range(NUM_TRIALS)]\n",
    "B_native = [random() for _ in range(NUM_TRIALS)]\n",
    "%timeit [a+b for a, b in zip(A_native, B_native)]\n",
    "\n",
    "print(\"\\nDecimal package:\")\n",
    "A_decimal = [Decimal(a) for a in A_native]\n",
    "B_decimal = [Decimal(b) for b in B_native]\n",
    "%timeit [a+b for a, b in zip(A_decimal, B_decimal)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The same and not the same.** Consider the following two program fragments:\n",
    "\n",
    "_Program 1_:\n",
    "\n",
    "    s = a - b\n",
    "    t = s + b\n",
    "    \n",
    "_Program 2_:\n",
    "\n",
    "    s = a + b\n",
    "    t = s - b\n",
    "\n",
    "Let $a=1.0$ and $b=\\epsilon_d / 2 \\approx 1.11 \\times 10^{-16}$, i.e., machine epsilon for IEEE-754 double-precision. Recall that we do not expect these programs to return the same value; let's run some Python code to see.\n",
    "\n",
    "> Note: The IEEE standard guarantees that given two finite-precision floating-point values, the result of applying any binary operator to them is the same as if the operator were applied in infinite-precision and then rounded back to finite-precision. The precise nature of rounding can be controlled by so-called _rounding modes_; the default rounding mode is \"[round-half-to-even](http://en.wikipedia.org/wiki/Rounding).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1: 0x1.fffffffffffffp-1\n",
      "t1: 0x1.0000000000000p+0\n",
      "\n",
      "\n",
      "s2: 0x1.0000000000000p+0\n",
      "t2: 0x1.fffffffffffffp-1\n",
      "\n",
      "1.0 vs. 0.9999999999999999\n",
      "(t1 == t2) == False\n"
     ]
    }
   ],
   "source": [
    "a = 1.0\n",
    "b = 2.**(-53) # == $\\epsilon_d$ / 2.0\n",
    "\n",
    "s1 =  a - b\n",
    "t1 = s1 + b\n",
    "\n",
    "s2 =  a + b\n",
    "t2 = s2 - b\n",
    "\n",
    "print(\"s1:\", s1.hex())\n",
    "print(\"t1:\", t1.hex())\n",
    "print(\"\\n\")\n",
    "print(\"s2:\", s2.hex())\n",
    "print(\"t2:\", t2.hex())\n",
    "\n",
    "print(\"\")\n",
    "print(t1, \"vs.\", t2)\n",
    "print(\"(t1 == t2) == {}\".format(t1 == t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, the NumPy/SciPy package, which we will cover later in the semester, allows you to determine machine epsilon in a portable way. Just note this fact for now.\n",
    "\n",
    "Here is an example of printing machine epsilon for both single-precision and double-precision values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-precision machine epsilon: 0x1.0000000000000p-23 ~ 1.1920929e-07\n",
      "Double-precision machine epsilon: 0x1.0000000000000p-52 ~ 2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "EPS_S = np.finfo (np.float32).eps\n",
    "EPS_D = np.finfo (float).eps\n",
    "\n",
    "print(\"Single-precision machine epsilon:\", float(EPS_S).hex(), \"~\", EPS_S)\n",
    "print(\"Double-precision machine epsilon:\", float(EPS_D).hex(), \"~\", EPS_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing floating-point programs\n",
    "\n",
    "Let's say someone devises an algorithm to compute $f(x)$. For a given value $x$, let's suppose this algorithm produces the value $\\mathrm{alg}(x)$. One important question might be, is that output \"good\" or \"bad?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward stability.** One way to show that the algorithm is good is to show that\n",
    "\n",
    "$$\n",
    "  \\left| \\mathrm{alg}(x) - f(x) \\right|\n",
    "$$\n",
    "\n",
    "is \"small\" for all $x$ of interest to your application. What is small depends on context. In any case, if you can show it then you can claim that the algorithm is _forward stable_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward stability.** Sometimes it is not easy to show forward stability directly. In such cases, you can also try a different technique, which is to show that the algorithm is, instead, _backward stable_.\n",
    "\n",
    "In particular, $\\mathrm{alg}(x)$ is a _backward stable algorithm_ to compute $f(x)$ if, for all $x$, there exists a \"small\" $\\Delta x$ such that\n",
    "\n",
    "$$\\mathrm{alg}(x) = f(x + \\Delta x).$$\n",
    "\n",
    "In other words, if you can show that the algorithm produces the exact answer to a slightly different input problem (meaning $\\Delta x$ is small, again in a context-dependent sense), then you can claim that the algorithm is backward stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Round-off errors.** You already know that numerical values can only be represented finitely, which introduces round-off error. Thus, at the very least we should hope that a scheme to compute $f(x)$ is as insensitive to round-off errors as possible. In other words, given that there will be round-off errors, if you can prove that $\\mathrm{alg}(x)$ is either forward or backward stable, then that will give you some measure of confidence that your algorithm is good.\n",
    "\n",
    "Here is the \"standard model\" of round-off error. Start by assuming that every scalar floating-point operation incurs some bounded error. That is, let $a \\odot b$ be the exact mathematical result of some operation on the inputs, $a$ and $b$, and let $\\mathrm{fl}(a \\odot b)$ be the _computed_ value, after rounding in finite-precision. The standard model says that\n",
    "\n",
    "$$\\mathrm{fl}(a \\odot b) \\equiv (a \\odot b) (1 + \\delta),$$\n",
    "\n",
    "where $|\\delta| \\leq \\epsilon$, machine epsilon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply these concepts on an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Computing a sum\n",
    "\n",
    "Let $x \\equiv (x_0, \\ldots, x_{n-1})$ be a collection of input data values. Suppose we wish to compute their sum.\n",
    "\n",
    "The exact mathematical result is\n",
    "\n",
    "$$\n",
    "  f(x) \\equiv \\sum_{i=0}^{n-1} x_i.\n",
    "$$\n",
    "\n",
    "Given $x$, let's also denote its exact sum by the synonym $s_{n-1} \\equiv f(x)$.\n",
    "  \n",
    "Now consider the following Python program to compute its sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alg_sum(x): # x == x[:n]\n",
    "    s = 0.\n",
    "    for x_i in x: # x_0, x_1, \\ldots, x_{n-1}\n",
    "        s += x_i\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In exact arithmetic, meaning without any rounding errors, this program would compute the exact sum. (See also the note below.) However, you know that finite arithmetic means there will be some rounding error after each addition.\n",
    "\n",
    "Let $\\delta_i$ denote the (unknown) error at iteration $i$. Then, assuming the collection `x` represents the input values exactly, you can show that `alg_sum(x)` computes $\\hat{s}_{n-1}$ where\n",
    "\n",
    "$$\n",
    "  \\hat{s}_{n-1} \\approx s_{n-1} + \\sum_{i=0}^{n-1} s_i \\delta_i,\n",
    "$$\n",
    "\n",
    "that is, the exact sum _plus_ a perturbation, which is the second term (the sum). The question, then, is under what conditions will this sum will be small?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a *backward error analysis*, you can show that\n",
    "\n",
    "$$\n",
    "  \\hat{s}_{n-1} \\approx \\sum_{i=0}^{n-1} x_i(1 + \\Delta_i) = f(x + \\Delta),\n",
    "$$\n",
    "\n",
    "where $\\Delta \\equiv (\\Delta_0, \\Delta_1, \\ldots, \\Delta_{n-1})$. In other words, the computed sum is the exact solution to a slightly different problem, $x + \\Delta$.\n",
    "\n",
    "To complete the analysis, you can at last show that\n",
    "\n",
    "$$\n",
    "  |\\Delta_i| \\leq (n-i) \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is machine precision. Thus, as long as $n \\epsilon \\ll 1$, then the algorithm is backward stable and you should expect the computed result to be close to the true result. Interpreted differently, as long as you are summing $n \\ll \\frac{1}{\\epsilon}$ values, then you needn't worry about the accuracy of the computed result compared to the true result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-precision: 1/epsilon_s ~= 8.4 million\n",
      "Double-precision: 1/epsilon_d ~= 4.5 quadrillion\n"
     ]
    }
   ],
   "source": [
    "print(\"Single-precision: 1/epsilon_s ~= {:.1f} million\".format(1e-6 / EPS_S))\n",
    "print(\"Double-precision: 1/epsilon_d ~= {:.1f} quadrillion\".format(1e-15 / EPS_D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this result, you can probably surmise why double-precision is usually the default in many languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of this summation, we can quantify not just the _backward error_ (i.e., $\\Delta_i$) but also the _forward error_. In that case, it turns out that\n",
    "\n",
    "$$\n",
    "  \\left| \\hat{s}_{n-1} - s_{n-1} \\right| \\lessapprox n \\epsilon \\|x\\|_1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note: Analysis in exact arithmetic.** We claimed above that `alg_sum()` is correct _in exact arithmetic_, i.e., in the absence of round-off error. You probably have a good sense of that just reading the code.\n",
    ">\n",
    "> However, if you wanted to argue about its correctness more formally, you might do so as follows using the technique of [proof by induction](https://en.wikipedia.org/wiki/Mathematical_induction). When your loops are more complicated and you want to prove that they are correct, you can often adapt this technique to your problem.\n",
    ">\n",
    "> First, assume that the `for` loop enumerates each element `p[i]` in order from `i=0` to `n-1`, where `n=len(p)`. That is, assume `p_i` is `p[i]`.\n",
    ">\n",
    "> Let $p_k \\equiv \\mathtt{p[}k\\mathtt{]}$ be the $k$-th element of `p[:]`. Let $s_i \\equiv \\sum_{k=0}^{i} p_k$; in other words, $s_i$ is the _exact_ mathematical sum of `p[:i+1]`. Thus, $s_{n-1}$ is the exact sum of `p[:]`.\n",
    ">\n",
    "> Let $\\hat{s}_{-1}$ denote the initial value of the variable `s`, which is 0. For any $i \\geq 0$, let $\\hat{s}_i$ denote the _computed_ value of the variable `s` immediately after the execution of line 4, where $i=\\mathtt{i}$. When $i=\\mathtt{i}=0$, $\\hat{s}_0 = \\hat{s}_{-1} + p_0 = p_0$, which is the exact sum of `p[:1]`. Thus, $\\hat{s}_0 = s_0$.\n",
    "> \n",
    "> Now suppose that $\\hat{s}_{i-1} = s_{i-1}$. When $\\mathtt{i}=i$, we want to show that $\\hat{s}_i = s_i$. After line 4 executes, $\\hat{s}_{i} = \\hat{s}_{i-1} + p_i = s_{i-1} + p_i = s_i$. Thus, the computed value $\\hat{s}_i$ is the exact sum $s_i$.\n",
    ">\n",
    "> If $i=n$, then, at line 5, the value $\\mathtt{s}=\\hat{s}_{n-1}=s_{n-1}$, and thus the program must in line 5 return the exact sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A numerical experiment: Summation\n",
    "\n",
    "Let's do an experiment to verify that these bounds hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "calc_summation.prompt"
    ]
   },
   "source": [
    "### Exercise 0: (2 points)\n",
    "**calc_summation**  \n",
    "\n",
    "**Your task:** define `calc_summation` as follows:\n",
    "\n",
    "In the code cell below, we've defined a list,\n",
    "\n",
    "```python\n",
    "    N = [10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "```\n",
    "\n",
    "* Take each entry `N[i]` to be a problem size.\n",
    "* Let `t[:len(N)]` be a list, which will hold computed sums.\n",
    "* For each `N[i]`, run an experiment where you sum a list of values `x[:N[i]]` using `alg_sum()`. You should initialize `x[:]` so that all elements have the value **`0.1`**. Store the computed sum in `t[i]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "tags": [
     "calc_summation.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999999999999999, 9.99999999999998, 99.9999999999986, 1000.0000000001588, 10000.000000018848, 100000.00000133288, 999999.9998389754]\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 0  \n",
    "def calc_summation(N):\n",
    "    # Initialize an array t of size len(N) to all zeroes.\n",
    "    t = [] \n",
    "    \n",
    "    #Loop through each value \\(n\\) in the list \\(N\\). \n",
    "    #For each iteration, generate a data set \\(x\\) of length \\(n\\). \n",
    "    #very element must be \\(0.1\\)\n",
    "    for n in N:\n",
    "        x = [0.1] * n\n",
    "        sum_val = alg_sum(x)\n",
    "        t.append(sum_val)\n",
    "    \n",
    "    return t\n",
    "        \n",
    "    \n",
    "\n",
    "    # Your code should do the experiment described above for\n",
    "    # each problem size N[i], and store the computed sum in t[i].\n",
    "   \n",
    "    \n",
    "    return t\n",
    "\n",
    "### Demo function call\n",
    "demo_ex0_N = [10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "result = calc_summation(demo_ex0_N)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "calc_summation.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "[0.9999999999999999,\n",
    " 9.99999999999998,\n",
    " 99.9999999999986,\n",
    " 1000.0000000001588,\n",
    " 10000.000000018848,\n",
    " 100000.00000133288,\n",
    " 999999.9998389754]\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for calc_summation (exercise 0). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_0",
     "locked": true,
     "points": 2,
     "solution": false
    },
    "tags": [
     "calc_summation.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.03 seconds\n",
      "memory after test: 0.03 MB\n",
      "memory peak during test: 1.10 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 0  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=calc_summation,\n",
    "              ex_name='calc_summation',\n",
    "              key=b'QAzDg1N4nIXXpt4UrYj7HBN_dzm4L1MJmb7XvQEgHqs=', \n",
    "              n_iter=20)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to calc_summation did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Computing dot products\n",
    "\n",
    "Let $x$ and $y$ be two vectors of length $n$, and denote their dot product by $f(x, y) \\equiv x^T y$.\n",
    "\n",
    "Now suppose we store the values of $x$ and $y$ _exactly_ in two Python arrays, `x[0:n]` and `y[0:n]`. Further suppose we compute their dot product by the program, `alg_dot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alg_dot (x, y):\n",
    "    p = [xi*yi for (xi, yi) in zip (x, y)]\n",
    "    s = alg_sum (p)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise 1** (OPTIONAL -- 0 points, not graded or collected). Show under what conditions `alg_dot()` is backward stable.\n",
    "\n",
    "> _Hint._ Let $(x_k, y_k)$ denote the exact values of the corresponding inputs, $(\\mathtt{x}[k], \\mathtt{y}[k])$. Then the true dot product, $x^T y = \\sum_{l=0}^{n-1} x_l y_l$. Next, let $\\hat{p}_k$ denote the $k$-th computed product, i.e., $\\hat{p}_k \\equiv x_k y_k (1 + \\gamma_k)$, where $\\gamma_k$ is the $k$-th round-off error and $|\\gamma_k| \\leq \\epsilon$. Then apply the results for `alg_sum()` to analyze `alg_dot()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer.** Following the hint, `alg_sum` will compute $\\hat{s}_{n-1}$ on the _computed_ inputs, $\\{\\hat{p}_k\\}$. Thus,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\hat{s}_{n-1}\n",
    "    & \\approx &\n",
    "      \\sum_{l=0}^{n-1} \\hat{p}_l (1 + \\Delta_l) \\\\\n",
    "    & = &\n",
    "      \\sum_{l=0}^{n-1} x_l y_l (1 + \\gamma_l) (1 + \\Delta_l) \\\\\n",
    "    & = &\n",
    "      \\sum_{l=0}^{n-1} x_l y_l (1 + \\gamma_l + \\Delta_l + \\gamma_l \\Delta_l).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Mathematically, this appears to be the exact dot product to an input in which $x$ is exact and $y$ is perturbed (or vice-versa). To argue that `alg_dot` is backward stable, we need to establish under what conditions the perturbation, $\\left| \\gamma_l + \\Delta_l + \\gamma_l \\Delta_l \\right|$, is \"small.\" Since $|\\gamma_l| \\leq \\epsilon$ and $|\\Delta_l| \\leq n \\epsilon$,\n",
    "\n",
    "$$\n",
    "  \\left| \\gamma_l + \\Delta_l + \\gamma_l \\Delta_l \\right|\n",
    "  \\leq | \\gamma_l | + | \\Delta_l | + |\\gamma_l| \\cdot |\\Delta_l|\n",
    "  \\leq (n+1) \\epsilon + \\mathcal{O}(n \\epsilon^2)\n",
    "  \\approx (n+1) \\epsilon.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## More accurate summation\n",
    "\n",
    "Suppose you wish to compute the sum, $s = x_0 + x_1 + x_2 + x_3$. Let's say you use the \"standard algorithm,\" which accumulates the terms one-by-one from left-to-right, as done by `alg_sum()` above.\n",
    "\n",
    "For the standard algorithm, let the $i$-th addition incur a roundoff error, $\\delta_i$. Then our usual error analysis would reveal that the absolute error in the computed sum, $\\hat{s}$, is approximately:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\hat{s} - s\n",
    "    & \\approx &\n",
    "      x_0(\\delta_0 + \\delta_1 + \\delta_2 + \\delta_3)\n",
    "      + x_1(\\delta_1 + \\delta_2 + \\delta_3)\n",
    "      + x_2(\\delta_2 + \\delta_3)\n",
    "      + x_3\\delta_3.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "And since $|\\delta_i| \\leq \\epsilon$, you would bound the absolute value of the error by,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\left| \\hat{s} - s \\right|\n",
    "    & \\lesssim &\n",
    "      (4|x_0| + 3|x_1| + 2|x_2| + 1|x_3|)\\epsilon.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Notice that $|x_0|$ is multiplied by 4, $|x_1|$ by 3, and so on.\n",
    "\n",
    "In general, if there are $n$ values to sum, the $|x_i|$ term will be multiplied by $n-i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "alg_sum_accurate.prompt"
    ]
   },
   "source": [
    "### Exercise 1: (3 points)\n",
    "**alg_sum_accurate**  \n",
    "\n",
    "**Your task:** define `alg_sum_accurate` as follows:\n",
    "\n",
    "Based on the preceding observation, implement a new summation function, `alg_sum_accurate(x)` that computes a more accurate sum than `alg_sum()`.\n",
    "\n",
    "> _Hint 1._ You do **not** need `Decimal()` in this problem. Some of you will try to use it, but it's not necessary. In fact, using it is likely to generate \"out-of-memory\" errors.\n",
    ">\n",
    "> _Hint 2._ Some of you will try to \"implement\" the error formula to somehow compensate for the round-off error. But that shouldn't make sense to do. (Why not? Because the formula above is a _bound_, not an exact formula.) Instead, the intent of this problem is to see if you can look at the formula and understand how to interpret it. That is, how does the formula tell you to perform the summation to make it more accurate than the conventional method?\n",
    ">\n",
    "> _Hint 3._ Some of you will look up fancy algorithms to compensate for error. That's great, because you may learn something deep in the process, but it's also not necessary. Again, see Hint 2.\n",
    ">\n",
    "> _Note._ The test cell will disallow certain \"shortcuts\" on this problem, namely, importing certain implementations of summation. Only basic Python is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "tags": [
     "alg_sum_accurate.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377690371.23587704\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 1  \n",
    "def alg_sum_accurate(x):\n",
    "    assert type(x) is list\n",
    "    #our task: define alg_sum_accurate as follows:\n",
    "\n",
    "    #sort the input by magnitude (ascending) to minimize round-off errors\n",
    "    sorted_x = sorted(x, key=abs)\n",
    "    \n",
    "    s=0\n",
    "    \n",
    "    for i in sorted_x:\n",
    "        s+=i\n",
    "    return s\n",
    "    \n",
    "    \n",
    "\n",
    "### Demo function call\n",
    "demo_ex1_x = utils.load_object_from_publicdata('demo_ex1_x.dill')\n",
    "\n",
    "result = alg_sum_accurate(demo_ex1_x)\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "alg_sum_accurate.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "377690371.23587704\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for alg_sum_accurate (exercise 1). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_1",
     "locked": true,
     "points": 3,
     "solution": false
    },
    "tags": [
     "alg_sum_accurate.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial memory usage: 0.00 MB\n",
      "Test duration: 0.06 seconds\n",
      "memory after test: 0.06 MB\n",
      "memory peak during test: 2.12 MB\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 1  \n",
    "\n",
    "\n",
    "from cse6040_devkit.tester_fw.testers import Tester\n",
    "from yaml import safe_load\n",
    "from time import time\n",
    "\n",
    "tracemalloc.start()\n",
    "mem_start, peak_start = tracemalloc.get_traced_memory()\n",
    "print(f\"initial memory usage: {mem_start/1024/1024:.2f} MB\")\n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    executor = dill.load(f)\n",
    "\n",
    "@run_with_timeout(error_threshold=200.0, warning_threshold=100.0)\n",
    "@suppress_stdout\n",
    "def execute_tests(**kwargs):\n",
    "    return executor(**kwargs)\n",
    "\n",
    "\n",
    "# Execute test\n",
    "start_time = time()\n",
    "passed, test_case_vars, e = execute_tests(func=alg_sum_accurate,\n",
    "              ex_name='alg_sum_accurate',\n",
    "              key=b'QAzDg1N4nIXXpt4UrYj7HBN_dzm4L1MJmb7XvQEgHqs=', \n",
    "              n_iter=20)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "duration = time() - start_time\n",
    "print(f\"Test duration: {duration:.2f} seconds\")\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "print(f\"memory after test: {current_memory/1024/1024:.2f} MB\")\n",
    "print(f\"memory peak during test: {peak_memory/1024/1024:.2f} MB\")\n",
    "tracemalloc.stop()\n",
    "if e: raise e\n",
    "assert passed, 'The solution to alg_sum_accurate did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Done!** You have reached the end of Part 1. There are no additional parts, so if you are satisfied, be sure to submit both parts, declare victory, and move on!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
